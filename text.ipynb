{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFzL3C9IyWQR"
      },
      "source": [
        "bag of words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXVCVW_OyWQU",
        "outputId": "1ead193d-a1fc-4200-e356-fcbc992e05b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\boyiq\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import linear_model\n",
        "from sklearn.manifold import TSNE\n",
        "import gzip\n",
        "import json\n",
        "import string\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gTEOLoey-jR",
        "outputId": "a1da7c58-7247-4c10-8d88-6a2f7911d24b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "# file_path = '/content/drive/My Drive/Digital_Music_5.json.gz'\n",
        "file_path = 'Digital_Music_5.json.gz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-gbIwXaayWQW"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with gzip.open(file_path, 'rb') as file:\n",
        "    for byte_line in file:\n",
        "        line = byte_line.decode('utf-8').strip()\n",
        "        review = json.loads(line)\n",
        "        data.append(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH05-zgByWQW",
        "outputId": "2afe9613-5df3-4ba7-ce7b-48bcfc3a81b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'overall': 5.0,\n",
              " 'vote': '3',\n",
              " 'verified': True,\n",
              " 'reviewTime': '06 3, 2013',\n",
              " 'reviewerID': 'A2TYZ821XXK2YZ',\n",
              " 'asin': '3426958910',\n",
              " 'style': {'Format:': ' Audio CD'},\n",
              " 'reviewerName': 'Garrett',\n",
              " 'reviewText': 'This is awesome to listen to, A must-have for all Slayer fans..sadly needed to be a triple disc set..They have so many hits!!',\n",
              " 'summary': 'Slayer Rules!',\n",
              " 'unixReviewTime': 1370217600}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NcpcU2FIyWQX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset = []\n",
        "for d in data:\n",
        "    if 'reviewText' not in d:\n",
        "        continue\n",
        "    if 'overall' not in d:\n",
        "        d['overall'] = 5.0\n",
        "    dataset.append({'reviewText':d['reviewText'], 'overall': d['overall']})\n",
        "df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6Al6_oEhyWQX"
      },
      "outputs": [],
      "source": [
        "# preprocess the dataset's reviewtext by removing punctuation, stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1mzKIp0yWQX",
        "outputId": "e25c9257-2204-4d56-b180-37bbe49296c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\boyiq\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "import re\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "htx7lz3ryWQX"
      },
      "outputs": [],
      "source": [
        "df.reviewText = df.reviewText.apply(lambda x : preprocess(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "efB2OopryWQY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Bag of Words with bigrams and limited features\n",
        "text_transformer = CountVectorizer(ngram_range=(1, 2),\n",
        "                                   max_features=1000)\n",
        "\n",
        "# TF-IDF Transformation\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# Pipeline: CountVectorizer -> TfidfTransformer\n",
        "pipeline = Pipeline([\n",
        "    ('vect', text_transformer),\n",
        "    ('tfidf', tfidf_transformer)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XBy0l0BqyWQY"
      },
      "outputs": [],
      "source": [
        "# apply the BOW vectorization and TF-IDF transformation\n",
        "X_text = pipeline.fit_transform(df.reviewText)\n",
        "# 9:1 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_text, df.overall, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_AJcXzVyWQZ"
      },
      "source": [
        "# K-Nearest Neighbors with Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxD2FYfGyWQZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Function to predict using cosine similarity with KNN\n",
        "def predict_with_cosine_similarity(X_train, y_train, X_test, k=10):\n",
        "    # Calculate cosine similarity matrix\n",
        "    similarity_matrix = cosine_similarity(X_test, X_train)\n",
        "\n",
        "    # Predict ratings\n",
        "    predictions = []\n",
        "    for similarity in similarity_matrix:\n",
        "        # Get top k indices of most similar items\n",
        "        top_k_indices = np.argsort(similarity)[-k:]\n",
        "        # Compute the mean of the k nearest neighbors\n",
        "        predictions.append(np.mean(y_train.iloc[top_k_indices]))\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3anFT_1yWQa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict ratings for test data\n",
        "y_pred_cosine = predict_with_cosine_similarity(X_train, y_train, X_test, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write the predicted ratings to a file\n",
        "with open('cosine_predictions.txt', 'w') as f:\n",
        "    for p in y_pred_cosine:\n",
        "        f.write(str(p) + '\\n')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.39787773389141073\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# calculate mse score pairwise\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse_knn = mean_squared_error(y_test, y_pred_cosine)\n",
        "print(mse_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klzXj8IAyWQa"
      },
      "source": [
        "# SVM on tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.49394870642030075\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create and train the SVM model\n",
        "svm_model = SVC(kernel='linear') # try other kenrnels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict ratings for test data\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
        "print(f\"SVM Model Accuracy: {mse_svm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87pPpftyWQa"
      },
      "source": [
        "# Multinomial Naive Bayes Classifier on TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LO39JAP23W3D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes mse: 0.5506101515062194, best alpha: 0.001\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "alpha = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0,100.0,1000.0]\n",
        "\n",
        "best_alpha = 0\n",
        "best_score = -10000000\n",
        "# 10-fold cross validation\n",
        "for alp in alpha:\n",
        "    nb_model = MultinomialNB(alpha=alp)\n",
        "    scores = cross_val_score(nb_model, X_train, y_train, cv=cv_num, scoring='neg_mean_squared_error')\n",
        "    score = np.mean(scores)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_alpha = alp\n",
        "nb_model = MultinomialNB(alpha=best_alpha)\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict ratings for test data\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "mse_nb = mean_squared_error(y_test, y_pred_nb)\n",
        "print(f\"Naive Bayes mse: {mse_nb}, best alpha: {best_alpha}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# linear Regressor on Bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge Regression mse: 0.39684467921719774, alpha =  5.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "alpha = [0.05, 0.1, 1,0,5.0, 10.0,100.0,1000.0]\n",
        "cv_num = 2\n",
        "\n",
        "\n",
        "best_alpha = 0\n",
        "best_score = -100000\n",
        "for alp in alpha:\n",
        "    ridge_model = Ridge(alpha=alp)\n",
        "    scores = cross_val_score(ridge_model, X_train, y_train, cv=cv_num, scoring='neg_mean_squared_error')\n",
        "    score = np.mean(scores)\n",
        "    # print(\"score\", score)\n",
        "    # print(\"alpha\", alp)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_alpha = alp\n",
        "\n",
        "ridge_model = Ridge(alpha=best_alpha)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "print(f\"Ridge Regression mse: {mse_ridge}, alpha =  {best_alpha}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\boyiq\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "C:\\Users\\boyiq\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:592: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15531.787545617954, tolerance: 3.9488917542250808\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "C:\\Users\\boyiq\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "C:\\Users\\boyiq\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:592: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15037.596734800121, tolerance: 3.855092001834141\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "C:\\Users\\boyiq\\AppData\\Local\\Temp\\ipykernel_21420\\4264180197.py:14: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  lasso_model.fit(X_train, y_train)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "best_alpha = 0\n",
        "best_score = -100000\n",
        "for alp in alpha:\n",
        "    lasso_model = Lasso(alpha=alp)\n",
        "    scores = cross_val_score(lasso_model, X_train, y_train, cv=cv_num, scoring='neg_mean_squared_error')\n",
        "    score = np.mean(scores)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_alpha = alp\n",
        "\n",
        "lasso_model = Lasso(alpha=best_alpha)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "print(f\"Lasso Regression mse: {mse_lasso}, alpha =  {best_alpha}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elastic Net Regression mse: 0.49258261376896556\n",
            "Best alpha: 100.0\n",
            "Best l1_ratio: 0.1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "set_of_alphas = [0.1, 0.5, 1.0, 10.0,100.0, 1000.0]\n",
        "set_of_l1_ratios = [0.1, 0.5, 0.9]\n",
        "best_score = -10000\n",
        "best_alpha = 0\n",
        "best_l1_ratio = 0\n",
        "for alpha in set_of_alphas:\n",
        "    for l1_ratio in set_of_l1_ratios:\n",
        "        elastic_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "        scores = cross_val_score(elastic_model, X_train, y_train, cv=cv_num, scoring='neg_mean_squared_error')\n",
        "        if scores.mean() > best_score:\n",
        "            best_score = scores.mean()\n",
        "            best_alpha = alpha\n",
        "            best_l1_ratio = l1_ratio\n",
        "elastic_model = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
        "elastic_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_elastic = elastic_model.predict(X_test)\n",
        "mse_elastic = mean_squared_error(y_test, y_pred_elastic)\n",
        "\n",
        "print(f\"Elastic Net Regression mse: {mse_elastic}\")\n",
        "print(f\"Best alpha: {best_alpha}\")\n",
        "print(f\"Best l1_ratio: {best_l1_ratio}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
