{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFzL3C9IyWQR"
      },
      "source": [
        "bag of words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXVCVW_OyWQU",
        "outputId": "1ead193d-a1fc-4200-e356-fcbc992e05b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\boyiq\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))\n",
        "from nltk.stem.porter import *\n",
        "from sklearn import linear_model\n",
        "from sklearn.manifold import TSNE\n",
        "import gzip\n",
        "import json\n",
        "import string\n",
        "import math\n",
        "\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gTEOLoey-jR",
        "outputId": "a1da7c58-7247-4c10-8d88-6a2f7911d24b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "# file_path = '/content/drive/My Drive/Digital_Music_5.json.gz'\n",
        "file_path = 'Digital_Music_5.json.gz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-gbIwXaayWQW"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with gzip.open(file_path, 'rb') as file:\n",
        "    for byte_line in file:\n",
        "        line = byte_line.decode('utf-8').strip()\n",
        "        review = json.loads(line)\n",
        "        data.append(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH05-zgByWQW",
        "outputId": "2afe9613-5df3-4ba7-ce7b-48bcfc3a81b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'overall': 5.0,\n",
              " 'vote': '3',\n",
              " 'verified': True,\n",
              " 'reviewTime': '06 3, 2013',\n",
              " 'reviewerID': 'A2TYZ821XXK2YZ',\n",
              " 'asin': '3426958910',\n",
              " 'style': {'Format:': ' Audio CD'},\n",
              " 'reviewerName': 'Garrett',\n",
              " 'reviewText': 'This is awesome to listen to, A must-have for all Slayer fans..sadly needed to be a triple disc set..They have so many hits!!',\n",
              " 'summary': 'Slayer Rules!',\n",
              " 'unixReviewTime': 1370217600}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NcpcU2FIyWQX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset = []\n",
        "for d in data:\n",
        "    if 'reviewText' not in d:\n",
        "        continue\n",
        "    if 'overall' not in d:\n",
        "        d['overall'] = 5.0\n",
        "    dataset.append({'reviewText':d['reviewText'], 'overall': d['overall']})\n",
        "df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6Al6_oEhyWQX"
      },
      "outputs": [],
      "source": [
        "# preprocess the dataset's reviewtext by removing punctuation, stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1mzKIp0yWQX",
        "outputId": "e25c9257-2204-4d56-b180-37bbe49296c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\boyiq\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "import re\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "htx7lz3ryWQX"
      },
      "outputs": [],
      "source": [
        "df.reviewText = df.reviewText.apply(lambda x : preprocess(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "efB2OopryWQY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Bag of Words with bigrams and limited features\n",
        "text_transformer = CountVectorizer(ngram_range=(1, 2),\n",
        "                                   max_features=1000)\n",
        "\n",
        "# TF-IDF Transformation\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# Pipeline: CountVectorizer -> TfidfTransformer\n",
        "pipeline = Pipeline([\n",
        "    ('vect', text_transformer),\n",
        "    ('tfidf', tfidf_transformer)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XBy0l0BqyWQY"
      },
      "outputs": [],
      "source": [
        "# apply the BOW vectorization and TF-IDF transformation\n",
        "X_text = pipeline.fit_transform(df.reviewText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2w8lH5aXyWQY"
      },
      "outputs": [],
      "source": [
        "# 9:1 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_text, df.overall, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF951kvYyWQZ",
        "outputId": "9af39097-b5fa-4aab-ec07-5f5c639ced02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<152660x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1530048 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UinewtBJyWQZ",
        "outputId": "665bff47-9242-4cad-84eb-c4aed6957f41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "134858    5.0\n",
              "104072    4.0\n",
              "115766    5.0\n",
              "66080     5.0\n",
              "19492     4.0\n",
              "         ... \n",
              "119879    1.0\n",
              "103694    5.0\n",
              "131932    5.0\n",
              "146867    5.0\n",
              "121958    5.0\n",
              "Name: overall, Length: 152660, dtype: float64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_AJcXzVyWQZ"
      },
      "source": [
        "# K-Nearest Neighbors with Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wxD2FYfGyWQZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Function to predict using cosine similarity with KNN\n",
        "def predict_with_cosine_similarity(X_train, y_train, X_test, k=10):\n",
        "    # Calculate cosine similarity matrix\n",
        "    similarity_matrix = cosine_similarity(X_test, X_train)\n",
        "\n",
        "    # Predict ratings\n",
        "    predictions = []\n",
        "    for similarity in similarity_matrix:\n",
        "        # Get top k indices of most similar items\n",
        "        top_k_indices = np.argsort(similarity)[-k:]\n",
        "        # Compute the mean of the k nearest neighbors\n",
        "        predictions.append(np.mean(y_train.iloc[top_k_indices]))\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N3anFT_1yWQa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict ratings for test data\n",
        "y_pred_cosine = predict_with_cosine_similarity(X_train, y_train, X_test, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write the predicted ratings to a file\n",
        "with open('cosine_predictions.txt', 'w') as f:\n",
        "    for p in y_pred_cosine:\n",
        "        f.write(str(p) + '\\n')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.39787773389141073\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# calculate mse score pairwise\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, y_pred_cosine)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klzXj8IAyWQa"
      },
      "source": [
        "# SVM on tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train the SVM model\n",
        "svm_model = SVC(kernel='linear') # try other kenrnels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict ratings for test data\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Model Accuracy: {accuracy_svm}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87pPpftyWQa"
      },
      "source": [
        "# Multinomial Naive Bayes Classifier on TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LO39JAP23W3D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes mse: 0.5565642869775393\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create and train the Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train) # default alpha=1.0, \n",
        "\n",
        "# Predict ratings for test data\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "mse_nb = mean_squared_error(y_test, y_pred_nb)\n",
        "print(f\"Naive Bayes mse: {mse_nb}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
